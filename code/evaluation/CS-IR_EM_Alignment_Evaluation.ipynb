{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b1cc59-89bf-41b7-a4bb-f7a9359d4fde",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491148f-4570-4004-9ec7-a2fce9350a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "=== Model: Alibaba-NLP/gte-multilingual-base ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESUME] Alibaba-NLP/gte-multilingual-base | D=en: already completed. Skipping.\n",
      "[RESUME] Alibaba-NLP/gte-multilingual-base | D=es: already completed. Skipping.\n",
      "[RESUME] Alibaba-NLP/gte-multilingual-base | D=zh: already completed. Skipping.\n",
      "\n",
      "=== Model: intfloat/multilingual-e5-small ===\n",
      "[RESUME] intfloat/multilingual-e5-small | D=en: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-small | D=es: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-small | D=zh: already completed. Skipping.\n",
      "\n",
      "=== Model: intfloat/multilingual-e5-base ===\n",
      "[RESUME] intfloat/multilingual-e5-base | D=en: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-base | D=es: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-base | D=zh: already completed. Skipping.\n",
      "\n",
      "=== Model: intfloat/multilingual-e5-large ===\n",
      "[RESUME] intfloat/multilingual-e5-large | D=en: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-large | D=es: already completed. Skipping.\n",
      "[RESUME] intfloat/multilingual-e5-large | D=zh: already completed. Skipping.\n",
      "\n",
      "=== Model: jinaai/jina-embeddings-v3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESUME] jinaai/jina-embeddings-v3 | D=en: already completed. Skipping.\n",
      "[RESUME] jinaai/jina-embeddings-v3 | D=es: already completed. Skipping.\n",
      "[RESUME] jinaai/jina-embeddings-v3 | D=zh: already completed. Skipping.\n",
      "\n",
      "=== Model: upskyy/bge-m3-korean ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5560f9d038894044b21632bb0a760544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c18a1941da4f9398bc30a07b44fe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e3c4e2a1b648f0bfee7b66afc1de78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eca4fc5dd1c45acbc9e69fe0f275e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25f15fa589545de8f1d9cede2f062aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4d12bff48142adba3c2fa28518a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68590175850942ccaad45f7efbdd4f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a684e9e9a80543d19ce746d6747be8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e26e2c653554110a1132da5f209b0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] upskyy/bge-m3-korean | D=en: computing aggregates…\n",
      "[DONE] upskyy/bge-m3-korean | D=en: wrote D_en_QQ.csv, D_en_QD.csv, D_en_Jaccard.csv\n",
      "[RUN] upskyy/bge-m3-korean | D=es: computing aggregates…\n",
      "[DONE] upskyy/bge-m3-korean | D=es: wrote D_es_QQ.csv, D_es_QD.csv, D_es_Jaccard.csv\n",
      "[RUN] upskyy/bge-m3-korean | D=zh: computing aggregates…\n",
      "[DONE] upskyy/bge-m3-korean | D=zh: wrote D_zh_QQ.csv, D_zh_QD.csv, D_zh_Jaccard.csv\n",
      "[Excel] Updated workbook → alignment_eval_results.xlsx\n",
      "All done (aggregates only, resumable).\n"
     ]
    }
   ],
   "source": [
    "# csir_alignment_eval_aggregates_only.py\n",
    "# pip install sentence-transformers torch pandas numpy tqdm xlsxwriter\n",
    "\n",
    "import os, json, math, re, gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ============== Config ==============\n",
    "# Paths (override with env vars if needed)\n",
    "QUERIES_PATH = os.getenv(\"QUERIES_PATH\", \"../Data/queries_subset.jsonl\")\n",
    "DOCS_PATH    = os.getenv(\"DOCS_PATH\",    \"../Data/docs_subset.jsonl\")\n",
    "RELS_PATH    = os.getenv(\"RELS_PATH\",    \"../Data/qrels_subset.jsonl\")\n",
    "\n",
    "# Output roots\n",
    "RUN_ROOT   = Path(\"./alignment_eval_runs\")         # per-(model, doc-lang) aggregate CSVs\n",
    "AGG_ROOT   = Path(\"./alignment_eval_aggregates\")   # global merged aggregate CSVs\n",
    "CACHE_ROOT = Path(\"./alignment_eval_cache\")        # cached embeddings\n",
    "EXCEL_PATH = Path(\"./alignment_eval_results.xlsx\") # single Excel with aggregate tables\n",
    "\n",
    "for p in (RUN_ROOT, AGG_ROOT, CACHE_ROOT):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models (Hugging Face IDs) — EMs only\n",
    "MODELS = [\n",
    "    \"Alibaba-NLP/gte-multilingual-base\",\n",
    "    \"intfloat/multilingual-e5-small\",\n",
    "    \"intfloat/multilingual-e5-base\",\n",
    "    \"intfloat/multilingual-e5-large\",\n",
    "    \"jinaai/jina-embeddings-v3\",\n",
    "    \"upskyy/bge-m3-korean\",\n",
    "]\n",
    "\n",
    "# Retrieval & eval\n",
    "TOPK = int(os.getenv(\"TOPK\", \"10\"))\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"256\"))\n",
    "SWITCH_BINS = [(0.0,0.2),(0.2,0.4),(0.4,0.6),(0.6,0.8),(0.8,1.0)]  # closed-left\n",
    "\n",
    "# CUDA settings\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if HAS_CUDA else \"cpu\"\n",
    "\n",
    "# ============== Utilities ==============\n",
    "def safe_model_slug(model_id: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.-]+\", \"_\", model_id.replace(\"/\", \"_\"))\n",
    "\n",
    "def read_jsonl(path: str) -> List[dict]:\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if ln:\n",
    "                out.append(json.loads(ln))\n",
    "    return out\n",
    "\n",
    "def load_data(q_path=QUERIES_PATH, d_path=DOCS_PATH, r_path=RELS_PATH):\n",
    "    queries = read_jsonl(q_path)\n",
    "    docs = read_jsonl(d_path)\n",
    "    rels = read_jsonl(r_path)\n",
    "    q_by_id = {str(q.get(\"id\", q.get(\"query_id\"))): q for q in queries}\n",
    "    d_by_id = {str(d.get(\"doc_id\")): d for d in docs}\n",
    "    rel_pos: Dict[str, set] = {}\n",
    "    for r in rels:\n",
    "        if r.get(\"relevance\", 0) > 0:\n",
    "            rel_pos.setdefault(str(r[\"query_id\"]), set()).add(str(r[\"doc_id\"]))\n",
    "    return queries, docs, rels, q_by_id, d_by_id, rel_pos\n",
    "\n",
    "def _norm_lang_alias(x: str) -> str:\n",
    "    x = (x or \"\").lower()\n",
    "    if x.startswith((\"es\",\"spa\")): return \"es\"\n",
    "    if x.startswith((\"en\",\"eng\")): return \"en\"\n",
    "    if x.startswith((\"zh\",\"chi\",\"zho\",\"cn\")): return \"zh\"\n",
    "    return x\n",
    "\n",
    "def _candidate_keys(obj_keys, lang_key, is_query):\n",
    "    aliases = {\n",
    "        \"en\": [\"en\",\"eng\",\"english\"],\n",
    "        \"es\": [\"es\",\"spa\",\"spanish\"],\n",
    "        \"zh\": [\"zh\",\"chi\",\"zho\",\"cn\",\"zh_cn\",\"zh-cn\",\"chinese\"],\n",
    "    }.get(lang_key, [lang_key])\n",
    "    cands = []\n",
    "    if is_query:\n",
    "        for a in aliases:\n",
    "            cands.append(f\"{a}_q\")\n",
    "    for a in aliases:\n",
    "        cands.extend([a, f\"{a}_doc\", f\"{a}_d\"])\n",
    "    if is_query:\n",
    "        pat = re.compile(rf\".*(?:^|_)(?:{'|'.join(map(re.escape, aliases))})(?:_|$).*_q$\")\n",
    "        for k in obj_keys:\n",
    "            if pat.match(k):\n",
    "                cands.append(k)\n",
    "    # dedupe keep order\n",
    "    seen, out = set(), []\n",
    "    for k in cands:\n",
    "        if k not in seen:\n",
    "            seen.add(k); out.append(k)\n",
    "    return out\n",
    "\n",
    "def _get_text_value(v):\n",
    "    if isinstance(v, str):\n",
    "        return v.strip()\n",
    "    if isinstance(v, dict):\n",
    "        t = (v.get(\"text\") or v.get(\"value\") or \"\").strip()\n",
    "        if t:\n",
    "            return t\n",
    "    return \"\"\n",
    "\n",
    "def texts_for_lang(items: List[dict], ids: List[str], lang_key: str, is_query: bool) -> List[str]:\n",
    "    id_key = \"query_id\" if is_query else \"doc_id\"\n",
    "    d = {str(it.get(id_key, it.get(\"id\"))): it for it in items}\n",
    "    out, empties = [], []\n",
    "    for id_ in ids:\n",
    "        obj = d[id_]\n",
    "        val = \"\"\n",
    "        if lang_key in obj:\n",
    "            val = _get_text_value(obj[lang_key])\n",
    "        if not val:\n",
    "            for ck in _candidate_keys(obj.keys(), lang_key, is_query):\n",
    "                if ck in obj:\n",
    "                    val = _get_text_value(obj[ck]); \n",
    "                    if val: break\n",
    "        if is_query and not val:\n",
    "            for k,v in obj.items():\n",
    "                if isinstance(k, str) and k.endswith(\"_q\"):\n",
    "                    val = _get_text_value(v)\n",
    "                    if val: break\n",
    "        if not val:\n",
    "            empties.append(id_); val=\"\"\n",
    "        out.append(val)\n",
    "    if empties:\n",
    "        print(f\"[WARN] {len(empties)} empty for lang='{lang_key}' is_query={is_query}. ex={empties[:5]}\")\n",
    "    return out\n",
    "\n",
    "def build_sw_rate_lookup(queries: List[dict]) -> Dict[Tuple[str, str], float]:\n",
    "    out: Dict[Tuple[str,str], float] = {}\n",
    "    for o in queries:\n",
    "        qid = str(o.get(\"id\") or o.get(\"query_id\") or \"\").strip()\n",
    "        if not qid: continue\n",
    "        for pair in (\"es_en\",\"zh_en\",\"zh_es\"):\n",
    "            q_key = f\"{pair}_q\"\n",
    "            sw_key = f\"{pair}_sw_rate\"\n",
    "            if q_key in o and isinstance(o[q_key], str) and o[q_key].strip():\n",
    "                val = o.get(sw_key, None)\n",
    "                if val is not None:\n",
    "                    try:\n",
    "                        out[(q_key, qid)] = float(val)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    return out\n",
    "\n",
    "# ============== Math helpers ==============\n",
    "def unit(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "def cos(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a, b = unit(a), unit(b)\n",
    "    return float(np.clip((a * b).sum(-1), -1.0, 1.0))\n",
    "\n",
    "def angle(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(math.acos(np.clip(cos(a,b), -1.0, 1.0)))  # radians\n",
    "\n",
    "def centroid(vecs: np.ndarray) -> np.ndarray:\n",
    "    if vecs.size == 0:\n",
    "        return vecs\n",
    "    return unit(unit(vecs).mean(axis=0))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def topk_indices(Q: np.ndarray, D: np.ndarray, k: int) -> np.ndarray:\n",
    "    dev = \"cuda\" if HAS_CUDA else \"cpu\"\n",
    "    dtype = torch.float16 if HAS_CUDA else torch.float32\n",
    "    Qt = torch.from_numpy(Q).to(dev, dtype=dtype, non_blocking=True)\n",
    "    Dt = torch.from_numpy(D).to(dev, dtype=dtype, non_blocking=True)\n",
    "    sims = (Qt @ Dt.T).to(torch.float32)\n",
    "    k = min(k, Dt.shape[0])\n",
    "    _, idx = torch.topk(sims, k=k, largest=True, sorted=True, dim=1)\n",
    "    out = idx.detach().cpu().numpy()\n",
    "    del Qt, Dt, sims\n",
    "    if HAS_CUDA: torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "# ============== Embedding (cached) ==============\n",
    "@dataclass\n",
    "class EncCfg:\n",
    "    model_id: str\n",
    "    batch_size: int = BATCH_SIZE\n",
    "\n",
    "def _cache_path(model_id: str, kind: str, lang: str) -> Path:\n",
    "    safe = safe_model_slug(model_id)\n",
    "    return CACHE_ROOT / f\"{safe}__{kind}_{lang}.npy\"\n",
    "\n",
    "def load_model(model_id: str) -> SentenceTransformer:\n",
    "    m = SentenceTransformer(model_id, device=DEVICE, trust_remote_code=True)\n",
    "    return m\n",
    "\n",
    "def encode_cached(model: SentenceTransformer, model_id: str, kind: str, lang: str, texts: List[str]) -> np.ndarray:\n",
    "    path = _cache_path(model_id, kind, lang)\n",
    "    if path.exists():\n",
    "        return np.load(path)\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "    ).astype(np.float32)\n",
    "    np.save(path, embs)\n",
    "    return embs\n",
    "\n",
    "# ============== Grouping / Binning / Accumulators ==============\n",
    "Q_LANG_KEYS = [\"en_q\",\"es_q\",\"zh_q\",\"es_en_q\",\"zh_en_q\",\"zh_es_q\"]\n",
    "DOC_LANGS   = [\"en\",\"es\",\"zh\"]\n",
    "\n",
    "def primary_lang_of(q_lang_key: str) -> str:\n",
    "    key = q_lang_key[:-2] if q_lang_key.endswith(\"_q\") else q_lang_key\n",
    "    first = key.split(\"_\")[0]\n",
    "    return _norm_lang_alias(first)\n",
    "\n",
    "def sw_bin_label(sw: Optional[float]) -> str:\n",
    "    if sw is None or (isinstance(sw, float) and (math.isnan(sw))):\n",
    "        return \"NA\"\n",
    "    x = float(sw)\n",
    "    for lo, hi in SWITCH_BINS:\n",
    "        # include exact 1.0 in the last bin\n",
    "        if (lo <= x < hi) or (math.isclose(x,1.0) and math.isclose(hi,1.0)):\n",
    "            return f\"{lo:.1f}-{hi:.1f}\"\n",
    "    return \"NA\"\n",
    "\n",
    "class MeanAccumulator:\n",
    "    \"\"\"Accumulate sums and counts per group key; emit means.\"\"\"\n",
    "    def __init__(self, fields: List[str]):\n",
    "        self.fields = fields\n",
    "        self.sums: Dict[Tuple, List[float]] = {}\n",
    "        self.counts: Dict[Tuple, int] = {}\n",
    "\n",
    "    def add(self, key: Tuple, values: Dict[str, float]):\n",
    "        if key not in self.sums:\n",
    "            self.sums[key] = [0.0]*len(self.fields)\n",
    "            self.counts[key] = 0\n",
    "        row = self.sums[key]\n",
    "        for i, f in enumerate(self.fields):\n",
    "            row[i] += float(values.get(f, 0.0))\n",
    "        self.counts[key] += 1\n",
    "\n",
    "    def to_dataframe(self, columns_prefix: Optional[List[str]] = None):\n",
    "        records = []\n",
    "        for key, sums in self.sums.items():\n",
    "            cnt = self.counts[key]\n",
    "            means = [ (v / cnt if cnt>0 else float('nan')) for v in sums ]\n",
    "            rec = dict()\n",
    "            # unpack key into group columns\n",
    "            if columns_prefix:\n",
    "                for name, val in zip(columns_prefix, key):\n",
    "                    rec[name] = val\n",
    "            else:\n",
    "                # unnamed tuple -> generic g1,g2...\n",
    "                for i, val in enumerate(key):\n",
    "                    rec[f\"g{i+1}\"] = val\n",
    "            for f, m in zip(self.fields, means):\n",
    "                rec[f] = m\n",
    "            rec[\"N\"] = cnt\n",
    "            records.append(rec)\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "# ---------- Resumability helpers ----------\n",
    "def model_run_dir(model_id: str) -> Path:\n",
    "    return RUN_ROOT / safe_model_slug(model_id)\n",
    "\n",
    "def out_csv_path(model_id: str, d_lang: str, kind: str) -> Path:\n",
    "    # kind in {\"QQ\",\"QD\",\"Jaccard\"}\n",
    "    return model_run_dir(model_id) / f\"D_{d_lang}_{kind}.csv\"\n",
    "\n",
    "def done_flag_path(model_id: str, d_lang: str) -> Path:\n",
    "    return model_run_dir(model_id) / f\"D_{d_lang}.done\"\n",
    "\n",
    "def mark_done(model_id: str, d_lang: str):\n",
    "    f = done_flag_path(model_id, d_lang)\n",
    "    f.parent.mkdir(parents=True, exist_ok=True)\n",
    "    f.write_text(\"ok\", encoding=\"utf-8\")\n",
    "\n",
    "def is_done(model_id: str, d_lang: str) -> bool:\n",
    "    return done_flag_path(model_id, d_lang).exists()\n",
    "\n",
    "# ---------- Excel builder (merge aggregate CSVs) ----------\n",
    "def rebuild_excel_and_global_csvs():\n",
    "    qq_list, qd_list, j_list = [], [], []\n",
    "    for mdir in RUN_ROOT.glob(\"*\"):\n",
    "        if not mdir.is_dir(): continue\n",
    "        for f in mdir.glob(\"D_*_QQ.csv\"):\n",
    "            try: qq_list.append(pd.read_csv(f))\n",
    "            except Exception: pass\n",
    "        for f in mdir.glob(\"D_*_QD.csv\"):\n",
    "            try: qd_list.append(pd.read_csv(f))\n",
    "            except Exception: pass\n",
    "        for f in mdir.glob(\"D_*_Jaccard.csv\"):\n",
    "            try: j_list.append(pd.read_csv(f))\n",
    "            except Exception: pass\n",
    "\n",
    "    AGG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    agg_qq = pd.concat(qq_list, ignore_index=True) if qq_list else pd.DataFrame()\n",
    "    agg_qd = pd.concat(qd_list, ignore_index=True) if qd_list else pd.DataFrame()\n",
    "    agg_j  = pd.concat(j_list,  ignore_index=True) if j_list  else pd.DataFrame()\n",
    "\n",
    "    if not agg_qq.empty: (AGG_ROOT / \"agg_query_query.csv\").write_text(agg_qq.to_csv(index=False), encoding=\"utf-8\")\n",
    "    if not agg_qd.empty: (AGG_ROOT / \"agg_query_docs.csv\").write_text(agg_qd.to_csv(index=False), encoding=\"utf-8\")\n",
    "    if not agg_j.empty:  (AGG_ROOT / \"agg_jaccard.csv\").write_text(agg_j.to_csv(index=False),  encoding=\"utf-8\")\n",
    "\n",
    "    with pd.ExcelWriter(EXCEL_PATH, engine=\"xlsxwriter\") as writer:\n",
    "        if not agg_qq.empty: agg_qq.to_excel(writer, sheet_name=\"QQ Aggregates\", index=False)\n",
    "        if not agg_qd.empty: agg_qd.to_excel(writer, sheet_name=\"QD Aggregates\", index=False)\n",
    "        if not agg_j.empty:  agg_j.to_excel(writer, sheet_name=\"Jaccard Aggregates\", index=False)\n",
    "\n",
    "        # Optional pivots\n",
    "        try:\n",
    "            if not agg_qq.empty:\n",
    "                pivot_eas = (agg_qq.groupby([\"model\",\"doc_lang\",\"query_lang_cs\"])\n",
    "                                   [[\"EAS\"]].mean().reset_index())\n",
    "                pivot_eas.to_excel(writer, sheet_name=\"Pivot EAS\", index=False)\n",
    "            if not agg_qd.empty:\n",
    "                pivot_cd = (agg_qd.groupby([\"model\",\"doc_lang\",\"query_lang_cs\"])\n",
    "                                   [[\"CentroidDrift@K\"]].mean().reset_index())\n",
    "                pivot_cd.to_excel(writer, sheet_name=\"Pivot CentroidDrift\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"[Excel] Updated workbook → {EXCEL_PATH}\")\n",
    "\n",
    "# ============== Main ==============\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    queries, docs, rels, q_by_id, d_by_id, rel_pos = load_data()\n",
    "    qids = [str(q.get(\"id\", q.get(\"query_id\"))) for q in queries]\n",
    "    dids = [str(d.get(\"doc_id\")) for d in docs]\n",
    "\n",
    "    # Pre-extract texts per language\n",
    "    DOC_LANGS = [\"en\",\"es\",\"zh\"]\n",
    "    Q_LANG_KEYS = [\"en_q\",\"es_q\",\"zh_q\",\"es_en_q\",\"zh_en_q\",\"zh_es_q\"]\n",
    "    docs_text_cache   = {dl: texts_for_lang(docs, dids, dl, is_query=False) for dl in DOC_LANGS}\n",
    "    queries_text_cache= {ql: texts_for_lang(queries, qids, ql, is_query=True) for ql in Q_LANG_KEYS}\n",
    "    sw_rate_map = build_sw_rate_lookup(queries)\n",
    "\n",
    "    doc_id_arr = np.array(dids, dtype=object)\n",
    "\n",
    "    for model_id in MODELS:\n",
    "        print(f\"\\n=== Model: {model_id} ===\")\n",
    "        mdir = model_run_dir(model_id)\n",
    "        mdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            model = load_model(model_id)\n",
    "\n",
    "            # Cache embeddings per language\n",
    "            D_by_lang: Dict[str, np.ndarray] = {}\n",
    "            for d_lang in DOC_LANGS:\n",
    "                D_by_lang[d_lang] = encode_cached(model, model_id, \"docs\", d_lang, docs_text_cache[d_lang])\n",
    "\n",
    "            Q_by_lang: Dict[str, np.ndarray] = {}\n",
    "            for q_lang in Q_LANG_KEYS:\n",
    "                Q_by_lang[q_lang] = encode_cached(model, model_id, \"queries\", q_lang, queries_text_cache[q_lang])\n",
    "\n",
    "            # Per doc language processing (resumable)\n",
    "            for d_lang in DOC_LANGS:\n",
    "                if is_done(model_id, d_lang):\n",
    "                    print(f\"[RESUME] {model_id} | D={d_lang}: already completed. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"[RUN] {model_id} | D={d_lang}: computing aggregates…\")\n",
    "\n",
    "                try:\n",
    "                    D = D_by_lang[d_lang]\n",
    "\n",
    "                    # Precompute TopK indices for all q_lang against this D\n",
    "                    topk_by_q: Dict[str, np.ndarray] = {}\n",
    "                    for q_lang in Q_by_lang.keys():\n",
    "                        topk_by_q[q_lang] = topk_indices(Q_by_lang[q_lang], D, TOPK)\n",
    "\n",
    "                    # Accumulators\n",
    "                    acc_QQ = MeanAccumulator(fields=[\"EAS\",\"angle_rad\",\"qq_euclid\"])\n",
    "                    acc_QD = MeanAccumulator(fields=[\"ΔQC@K\",\"CentroidDrift@K\",\"AlignDrift@K\"])\n",
    "                    acc_J  = MeanAccumulator(fields=[\"Jaccard@K\"])\n",
    "\n",
    "                    # --- Query–Query & Query–Docs alignment ---\n",
    "                    for q_lang_cs in [\"es_en_q\",\"zh_en_q\",\"zh_es_q\"]:\n",
    "                        q_lang_mono = primary_lang_of(q_lang_cs) + \"_q\"\n",
    "                        Q_cs   = Q_by_lang[q_lang_cs]\n",
    "                        Q_mono = Q_by_lang[q_lang_mono]\n",
    "                        T_cs   = topk_by_q[q_lang_cs]\n",
    "                        T_mono = topk_by_q[q_lang_mono]\n",
    "\n",
    "                        for i, qid in enumerate(qids):\n",
    "                            q_cs   = Q_cs[i]\n",
    "                            q_m    = Q_mono[i]\n",
    "                            EAS    = cos(q_cs, q_m)\n",
    "                            ang    = angle(q_cs, q_m)\n",
    "                            qqL2   = float(np.linalg.norm(unit(q_cs) - unit(q_m)))\n",
    "\n",
    "                            idx_m = T_mono[i]\n",
    "                            idx_c = T_cs[i]\n",
    "                            C_m   = centroid(D[idx_m])\n",
    "                            C_c   = centroid(D[idx_c])\n",
    "\n",
    "                            QC_m  = cos(q_m, C_m)\n",
    "                            QC_c  = cos(q_cs, C_c)\n",
    "                            d_QC  = QC_c - QC_m\n",
    "                            c_drift = float(np.linalg.norm(unit(C_c) - unit(C_m)))\n",
    "                            q_drift = unit(q_cs) - unit(q_m)\n",
    "                            n_drift = unit(C_c)  - unit(C_m)\n",
    "                            align_drift = cos(q_drift, n_drift) if (np.linalg.norm(q_drift)>0 and np.linalg.norm(n_drift)>0) else 0.0\n",
    "\n",
    "                            sw = sw_rate_map.get((q_lang_cs, qid), np.nan)\n",
    "                            swb = sw_bin_label(sw)\n",
    "\n",
    "                            # Group keys\n",
    "                            key_QQ = (model_id, d_lang, primary_lang_of(q_lang_cs), q_lang_cs.replace(\"_\",\"-\").replace(\"-q\",\"\"), swb)\n",
    "                            key_QD = (model_id, d_lang, q_lang_cs.replace(\"_\",\"-\").replace(\"-q\",\"\"), swb)\n",
    "\n",
    "                            acc_QQ.add(key_QQ, {\"EAS\":EAS, \"angle_rad\":ang, \"qq_euclid\":qqL2})\n",
    "                            acc_QD.add(key_QD, {\"ΔQC@K\":d_QC, \"CentroidDrift@K\":c_drift, \"AlignDrift@K\":align_drift})\n",
    "\n",
    "                    # --- Jaccard@K (set overlap) ---\n",
    "                    for base, cs in [(\"en_q\",\"es_en_q\"), (\"en_q\",\"zh_en_q\"),\n",
    "                                     (\"es_q\",\"es_en_q\"), (\"zh_q\",\"zh_en_q\"),\n",
    "                                     (\"zh_q\",\"zh_es_q\"), (\"es_q\",\"zh_es_q\")]:\n",
    "                        T_b = topk_by_q[base]\n",
    "                        T_c = topk_by_q[cs]\n",
    "                        base_lbl = base.replace(\"_\",\"-\").replace(\"-q\",\"\")\n",
    "                        cs_lbl   = cs.replace(\"_\",\"-\").replace(\"-q\",\"\")\n",
    "                        for i, qid in enumerate(qids):\n",
    "                            set_b = set(doc_id_arr[T_b[i]])\n",
    "                            set_c = set(doc_id_arr[T_c[i]])\n",
    "                            t = len(set_b & set_c)\n",
    "                            j = t / (2*TOPK - t) if TOPK > 0 else 0.0\n",
    "                            sw = sw_rate_map.get((cs, qid), np.nan)\n",
    "                            swb = sw_bin_label(sw)\n",
    "                            key_J = (model_id, d_lang, base_lbl, cs_lbl, swb)\n",
    "                            acc_J.add(key_J, {\"Jaccard@K\": j})\n",
    "\n",
    "                    # ---- Write (overwrite) per-(model,doc-lang) aggregate CSVs\n",
    "                    qq_df = acc_QQ.to_dataframe(columns_prefix=[\"model\",\"doc_lang\",\"query_lang_mono\",\"query_lang_cs\",\"sw_bin\"])\n",
    "                    qd_df = acc_QD.to_dataframe(columns_prefix=[\"model\",\"doc_lang\",\"query_lang_cs\",\"sw_bin\"])\n",
    "                    j_df  = acc_J.to_dataframe(columns_prefix=[\"model\",\"doc_lang\",\"query_lang_mono\",\"query_lang_cs\",\"sw_bin\"])\n",
    "\n",
    "                    out_qq = out_csv_path(model_id, d_lang, \"QQ\")\n",
    "                    out_qd = out_csv_path(model_id, d_lang, \"QD\")\n",
    "                    out_j  = out_csv_path(model_id, d_lang, \"Jaccard\")\n",
    "\n",
    "                    qq_df.to_csv(out_qq, index=False, encoding=\"utf-8\")\n",
    "                    qd_df.to_csv(out_qd, index=False, encoding=\"utf-8\")\n",
    "                    j_df.to_csv(out_j,  index=False, encoding=\"utf-8\")\n",
    "\n",
    "                    mark_done(model_id, d_lang)\n",
    "                    print(f\"[DONE] {model_id} | D={d_lang}: wrote {out_qq.name}, {out_qd.name}, {out_j.name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {model_id} | D={d_lang}: {e}. Keeping partial results; continuing.\")\n",
    "                    gc.collect()\n",
    "                    if HAS_CUDA: torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] model load/run failed for {model_id}: {e}. Continuing with next model.\")\n",
    "        finally:\n",
    "            try:\n",
    "                del model\n",
    "            except Exception:\n",
    "                pass\n",
    "            gc.collect()\n",
    "            if HAS_CUDA: torch.cuda.empty_cache()\n",
    "\n",
    "    # Rebuild global CSVs + Excel (aggregates only)\n",
    "    rebuild_excel_and_global_csvs()\n",
    "    print(\"All done (aggregates only, resumable).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dce8f-5711-4ab2-9a7e-07640a56e0fa",
   "metadata": {},
   "source": [
    "# EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe173799-9b32-464b-9380-b97f6dcd669f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] Reading aggregates from: alignment_eval_runs\n",
      "[OK] Wrote report → alignment_alignment_report.xlsx\n",
      "  - QQ (by CS pair): 21 rows\n",
      "  - QQ (by CS pair & bin): 105 rows\n",
      "  - QD Shift (by CS pair): 21 rows\n",
      "  - QD Shift (by CS pair & bin): 105 rows\n"
     ]
    }
   ],
   "source": [
    "# build_alignment_report.py\n",
    "# pip install pandas numpy xlsxwriter\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --------- Config ---------\n",
    "RUN_ROOT   = Path(os.getenv(\"RUN_ROOT\", \"./alignment_eval_runs\"))   # where per-model CSVs live\n",
    "EXCEL_PATH = Path(os.getenv(\"EXCEL_PATH\", \"./alignment_alignment_report.xlsx\"))\n",
    "\n",
    "# --------- Load helpers ---------\n",
    "def _load_all(kind: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    kind in {\"QQ\",\"QD\"} or \"Jaccard\" (we don't need Jaccard for these 4 sheets, but kept for completeness).\n",
    "    Reads all D_*_<kind>.csv from subfolders of RUN_ROOT.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    if not RUN_ROOT.exists():\n",
    "        return pd.DataFrame()\n",
    "    for mdir in RUN_ROOT.glob(\"*\"):\n",
    "        if not mdir.is_dir():\n",
    "            continue\n",
    "        for f in mdir.glob(f\"D_*_{kind}.csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(f)\n",
    "                frames.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] failed reading {f}: {e}\")\n",
    "    if frames:\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in input for this sheet: {missing}\")\n",
    "\n",
    "# --------- Build sheets ---------\n",
    "def build_sheet_1_qq(df_qq: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sheet 1: Query–Query results per CS pair (lang1 -> lang2) and model (aggregated across doc_lang and sw_bin)\n",
    "    Metrics: EAS, angle_rad, qq_euclid, plus N\n",
    "    \"\"\"\n",
    "    need = [\"model\",\"query_lang_mono\",\"query_lang_cs\",\"EAS\",\"angle_rad\",\"qq_euclid\",\"N\"]\n",
    "    _ensure_cols(df_qq, need[:-1])  # N may or may not exist; we'll compute\n",
    "    # Build a 'pair' column like \"es→es-en\"\n",
    "    df = df_qq.copy()\n",
    "    df[\"pair\"] = df[\"query_lang_mono\"].astype(str) + \"→\" + df[\"query_lang_cs\"].astype(str)\n",
    "    # If N column exists from accumulator, keep it; otherwise add 1\n",
    "    if \"N\" not in df.columns:\n",
    "        df[\"N\"] = 1\n",
    "    # Aggregate across doc_lang and sw_bin\n",
    "    grp = df.groupby([\"model\",\"pair\"], as_index=False).agg({\n",
    "        \"EAS\":\"mean\",\n",
    "        \"angle_rad\":\"mean\",\n",
    "        \"qq_euclid\":\"mean\",\n",
    "        \"N\":\"sum\"\n",
    "    })\n",
    "    # Order columns nicely\n",
    "    return grp[[\"model\",\"pair\",\"EAS\",\"angle_rad\",\"qq_euclid\",\"N\"]].sort_values([\"model\",\"pair\"])\n",
    "\n",
    "def build_sheet_2_qq_bins(df_qq: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sheet 2: Query–Query results per CS pair & switch-rate bin (and model)\n",
    "    \"\"\"\n",
    "    need = [\"model\",\"query_lang_mono\",\"query_lang_cs\",\"sw_bin\",\"EAS\",\"angle_rad\",\"qq_euclid\",\"N\"]\n",
    "    _ensure_cols(df_qq, need[:-1])\n",
    "    df = df_qq.copy()\n",
    "    df[\"pair\"] = df[\"query_lang_mono\"].astype(str) + \"→\" + df[\"query_lang_cs\"].astype(str)\n",
    "    if \"N\" not in df.columns:\n",
    "        df[\"N\"] = 1\n",
    "    grp = df.groupby([\"model\",\"pair\",\"sw_bin\"], as_index=False).agg({\n",
    "        \"EAS\":\"mean\",\n",
    "        \"angle_rad\":\"mean\",\n",
    "        \"qq_euclid\":\"mean\",\n",
    "        \"N\":\"sum\"\n",
    "    })\n",
    "    # Keep NA bin last by simple sort trick\n",
    "    grp[\"sw_bin\"] = grp[\"sw_bin\"].fillna(\"NA\").astype(str)\n",
    "    return grp[[\"model\",\"pair\",\"sw_bin\",\"EAS\",\"angle_rad\",\"qq_euclid\",\"N\"]].sort_values([\"model\",\"pair\",\"sw_bin\"])\n",
    "\n",
    "def build_sheet_3_qd(df_qd: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sheet 3: Query–Docs alignment shift (monolingual vs CS) by model & CS pair (aggregated across doc_lang and sw_bin)\n",
    "    Metrics: ΔQC@K, CentroidDrift@K, AlignDrift@K, N\n",
    "    \"\"\"\n",
    "    need = [\"model\",\"query_lang_cs\",\"ΔQC@K\",\"CentroidDrift@K\",\"AlignDrift@K\",\"N\"]\n",
    "    _ensure_cols(df_qd, need[:-1])\n",
    "    df = df_qd.copy()\n",
    "    # For readability, expose a 'pair' = \"<mono>→<cs>\" using the first tag in cs\n",
    "    # The mono language is the first token of query_lang_cs (e.g., \"es-en\" -> \"es\")\n",
    "    # but we didn't carry mono explicitly in QD; we keep cs label as the 'pair' for this sheet.\n",
    "    # If you want explicit mono, uncomment the heuristic below:\n",
    "    # mono = df['query_lang_cs'].astype(str).str.split('-', n=1).str[0]\n",
    "    # df[\"pair\"] = mono + \"→\" + df[\"query_lang_cs\"].astype(str)\n",
    "    df[\"pair\"] = df[\"query_lang_cs\"].astype(str)\n",
    "\n",
    "    if \"N\" not in df.columns:\n",
    "        df[\"N\"] = 1\n",
    "    grp = df.groupby([\"model\",\"pair\"], as_index=False).agg({\n",
    "        \"ΔQC@K\":\"mean\",\n",
    "        \"CentroidDrift@K\":\"mean\",\n",
    "        \"AlignDrift@K\":\"mean\",\n",
    "        \"N\":\"sum\"\n",
    "    })\n",
    "    return grp[[\"model\",\"pair\",\"ΔQC@K\",\"CentroidDrift@K\",\"AlignDrift@K\",\"N\"]].sort_values([\"model\",\"pair\"])\n",
    "\n",
    "def build_sheet_4_qd_bins(df_qd: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sheet 4: Query–Docs alignment shift by model, CS pair & sw_bin\n",
    "    \"\"\"\n",
    "    need = [\"model\",\"query_lang_cs\",\"sw_bin\",\"ΔQC@K\",\"CentroidDrift@K\",\"AlignDrift@K\",\"N\"]\n",
    "    _ensure_cols(df_qd, need[:-1])\n",
    "    df = df_qd.copy()\n",
    "    # See note in sheet 3 about 'pair'; keep cs label\n",
    "    df[\"pair\"] = df[\"query_lang_cs\"].astype(str)\n",
    "    if \"N\" not in df.columns:\n",
    "        df[\"N\"] = 1\n",
    "    df[\"sw_bin\"] = df[\"sw_bin\"].fillna(\"NA\").astype(str)\n",
    "    grp = df.groupby([\"model\",\"pair\",\"sw_bin\"], as_index=False).agg({\n",
    "        \"ΔQC@K\":\"mean\",\n",
    "        \"CentroidDrift@K\":\"mean\",\n",
    "        \"AlignDrift@K\":\"mean\",\n",
    "        \"N\":\"sum\"\n",
    "    })\n",
    "    return grp[[\"model\",\"pair\",\"sw_bin\",\"ΔQC@K\",\"CentroidDrift@K\",\"AlignDrift@K\",\"N\"]].sort_values([\"model\",\"pair\",\"sw_bin\"])\n",
    "\n",
    "def main():\n",
    "    print(f\"[Load] Reading aggregates from: {RUN_ROOT}\")\n",
    "\n",
    "    df_qq = _load_all(\"QQ\")\n",
    "    df_qd = _load_all(\"QD\")\n",
    "\n",
    "    if df_qq.empty and df_qd.empty:\n",
    "        raise SystemExit(\"No aggregate CSVs found under RUN_ROOT. Run your aggregate eval first.\")\n",
    "\n",
    "    # Build sheets\n",
    "    sheets = {}\n",
    "    if not df_qq.empty:\n",
    "        sheets[\"QQ (by CS pair)\"] = build_sheet_1_qq(df_qq)\n",
    "        sheets[\"QQ (by CS pair & bin)\"] = build_sheet_2_qq_bins(df_qq)\n",
    "    else:\n",
    "        print(\"[Info] No QQ CSVs found; QQ sheets will be omitted.\")\n",
    "\n",
    "    if not df_qd.empty:\n",
    "        sheets[\"QD Shift (by CS pair)\"] = build_sheet_3_qd(df_qd)\n",
    "        sheets[\"QD Shift (by CS pair & bin)\"] = build_sheet_4_qd_bins(df_qd)\n",
    "    else:\n",
    "        print(\"[Info] No QD CSVs found; QD sheets will be omitted.\")\n",
    "\n",
    "    # Write Excel\n",
    "    EXCEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(EXCEL_PATH, engine=\"xlsxwriter\") as writer:\n",
    "        for name, df in sheets.items():\n",
    "            # Ensure <= Excel size limits\n",
    "            max_rows, max_cols = 1_048_576, 16_384\n",
    "            r, c = df.shape\n",
    "            if r > max_rows or c > max_cols:\n",
    "                raise ValueError(f\"Sheet '{name}' too large for Excel: {r}x{c}\")\n",
    "            df.to_excel(writer, sheet_name=name[:31], index=False)  # Excel sheet name <=31 chars\n",
    "\n",
    "    print(f\"[OK] Wrote report → {EXCEL_PATH}\")\n",
    "    for name, df in sheets.items():\n",
    "        print(f\"  - {name}: {len(df)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968bd48-bcda-432b-ac4d-af0c1f638ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Vir (csir)",
   "language": "python",
   "name": "csir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
