{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ebc971-7377-4ef5-b0ee-1543e7910ede",
   "metadata": {},
   "source": [
    "# GENERAL FUNCTIONS AND INITIALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9170af-a0b0-44c1-a2f2-8460bd65cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d85819-fe6d-49be-a0e0-90eadd7c0280",
   "metadata": {},
   "source": [
    "Separate chinese words for correct alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d67a721-214b-4477-8ee0-2f3f4f9a58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vir\\anaconda3\\envs\\embedding\\Lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pkg_resources\n",
    "\n",
    "def add_spaces_to_chinese(text):\n",
    "    words = jieba.cut(text)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f99a5-c1c2-4f5a-9d83-32bf962c17e2",
   "metadata": {},
   "source": [
    "Spacy NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f419689-fdb1-46a1-bdb4-41e74915ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a664f2-b531-4e25-b592-230d6eac54c9",
   "metadata": {},
   "source": [
    "Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d3cef3-a55c-4743-b21d-797f18b94978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model = AutoModel.from_pretrained(\"aneuraz/awesome-align-with-co\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aneuraz/awesome-align-with-co\")\n",
    "\n",
    "def aligner(src, tgt):\n",
    "    # model parameters\n",
    "    align_layer = 8\n",
    "    threshold = 1e-3\n",
    "    \n",
    "    # pre-processing\n",
    "    sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
    "    token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
    "    wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
    "    ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
    "    sub2word_map_src = []\n",
    "    for i, word_list in enumerate(token_src):\n",
    "      sub2word_map_src += [i for x in word_list]\n",
    "    sub2word_map_tgt = []\n",
    "    for i, word_list in enumerate(token_tgt):\n",
    "      sub2word_map_tgt += [i for x in word_list]\n",
    "      \n",
    "    # alignment\n",
    "    align_layer = 8\n",
    "    threshold = 1e-3\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "      out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "    \n",
    "      dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
    "    \n",
    "      softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
    "      softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
    "    \n",
    "      softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
    "    \n",
    "    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
    "    align_words = set()\n",
    "    for i, j in align_subwords:\n",
    "      align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
    "\n",
    "    sorted_align_words = sorted(align_words, key=lambda x: x[0])\n",
    "      \n",
    "    return sorted_align_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5464b0-61b4-44fb-8ea6-b492c6afd360",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776127e8-ab2e-4e4a-ac0f-d47963a59bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean spacing (safe for Chinese and spanish)\n",
    "def clean(text):\n",
    "    text = re.sub(r'\\s+([?.!,])', r'\\1', text)  # spanish punctuation\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63cb7-8596-4120-97cf-12527e0c8a91",
   "metadata": {},
   "source": [
    "Remove spaces in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602ba670-f0de-4eca-818e-1334e3999767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_spaces_between_chinese_and_punctuation(text):\n",
    "    chinese = r'[\\u4e00-\\u9fff]'\n",
    "    punct = r'[，。！？；、“”‘’：（）]'\n",
    "    latin = r'[A-Za-z]'\n",
    "\n",
    "    # Remove spaces between Chinese characters\n",
    "    text = re.sub(f'({chinese})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "\n",
    "    # Remove spaces between Chinese and Chinese punctuation\n",
    "    text = re.sub(f'({chinese})\\\\s+({punct})', r'\\1\\2', text)\n",
    "    text = re.sub(f'({punct})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "\n",
    "    # Remove spaces between Chinese and spanish (in both directions)\n",
    "    text = re.sub(f'({chinese})\\\\s+({latin})', r'\\1\\2', text)\n",
    "    text = re.sub(f'({latin})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "\n",
    "    # Repeat until no matches (for chained corrections)\n",
    "    pattern_list = [\n",
    "        f'({chinese})\\\\s+({chinese})',\n",
    "        f'({chinese})\\\\s+({punct})',\n",
    "        f'({punct})\\\\s+({chinese})',\n",
    "        f'({chinese})\\\\s+({latin})',\n",
    "        f'({latin})\\\\s+({chinese})'\n",
    "    ]\n",
    "\n",
    "    while any(re.search(p, text) for p in pattern_list):\n",
    "        text = re.sub(f'({chinese})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "        text = re.sub(f'({chinese})\\\\s+({punct})', r'\\1\\2', text)\n",
    "        text = re.sub(f'({punct})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "        text = re.sub(f'({chinese})\\\\s+({latin})', r'\\1\\2', text)\n",
    "        text = re.sub(f'({latin})\\\\s+({chinese})', r'\\1\\2', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae45e6e-8b20-4720-9f7f-32b3e9825854",
   "metadata": {},
   "source": [
    "Fix duplicate punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "284fe7ea-c00e-454c-9f34-d07445bd55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_fix_duplicate_punctuation(text):\n",
    "    # Match Chinese and spanish punctuation characters\n",
    "    punct = r'[，。！？；：,.!?;:]'\n",
    "\n",
    "    # Pattern to match duplicated punctuation with optional spaces (e.g. \"， ,\" or \"！ ！\")\n",
    "    pattern = re.compile(f'({punct})(\\\\s*{punct})+')\n",
    "\n",
    "    # Replace with a single occurrence of the first punctuation\n",
    "    text = pattern.sub(lambda m: m.group(1), text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376192c-fa08-4fcd-b87f-7151e5ebeac1",
   "metadata": {},
   "source": [
    "# INTRA-SWITCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ebb52-ac7b-4224-b716-9d1ac816e417",
   "metadata": {},
   "source": [
    "## PHRASE-LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d189ebbc-0b3e-4900-a28e-0edfad4d1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_boundaries(doc):\n",
    "    # Find 'CCONJ' tokens that join phrases (not simple compound nouns)\n",
    "    boundaries = []\n",
    "\n",
    "    last_b = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ' or token.pos_ == 'PUNCT':\n",
    "            # If the token is connecting ROOTs or verbs, it's phrase-separating\n",
    "            if (token.head.pos_ in {'VERB', 'AUX'} or token.head.dep_ == 'ROOT'):\n",
    "                #print(f\"'{token.text}' at position {token.i} separates phrases.\")\n",
    "                if (token.i)-1 != last_b:\n",
    "                    boundaries.append(token.i)\n",
    "                last_b = token.i\n",
    "    \n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80587200-df4c-44a6-849b-0e59c288c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_by_token_positions(doc, positions):\n",
    "    \"\"\"\n",
    "    Split a spaCy Doc object at specified token positions.\n",
    "\n",
    "    Parameters:\n",
    "        doc (spacy.tokens.Doc): Tokenized spaCy Doc object.\n",
    "        positions (List[int]): Token indices (including punctuation).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of string chunks split at token boundaries.\n",
    "    \"\"\"\n",
    "    tokens = [token.text_with_ws for token in doc]  # Preserve spacing and punctuation\n",
    "    chunks = []\n",
    "    prev = 0\n",
    "    for pos in positions:\n",
    "        chunks.append(''.join(tokens[prev:pos]))\n",
    "        prev = pos\n",
    "    chunks.append(''.join(tokens[prev:]))  # Add last segment\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff7b82b-f1d5-468f-9c7b-d791caf960b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_chunk_alignment(chunks_src, chunks_tgt, min_alignments=1):\n",
    "    \"\"\"\n",
    "    Check whether two lists of chunks are aligned using awesome-align.\n",
    "\n",
    "    Parameters:\n",
    "        chunks_src (List[str]): Source language chunks (e.g., Chinese).\n",
    "        chunks_tgt (List[str]): Target language chunks (e.g., spanish).\n",
    "        min_alignments (int): Minimum aligned word pairs required per chunk pair.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all chunk pairs are aligned with at least `min_alignments` alignments.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    if len(chunks_src) != len(chunks_tgt):\n",
    "        print(\"Not same number of chunks\")\n",
    "        return False\n",
    "\n",
    "    for zh_chunk, en_chunk in zip(chunks_src, chunks_tgt):\n",
    "        if len(en_chunk) > 1 and len(zh_chunk) > 1:\n",
    "            alignments = aligner(en_chunk, zh_chunk)\n",
    "            if len(alignments) < min_alignments:\n",
    "                print(\"Not minimum alignment\")\n",
    "                return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29550195-7afb-4a0d-854a-e03c6355ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_switch_phrase_bilingual(phrase_zh, phrase_es):\n",
    "    phrase_zh = add_spaces_to_chinese(phrase_zh)\n",
    "    doc_src = nlp_zh(phrase_zh)\n",
    "    doc_tgt = nlp_es(phrase_es)\n",
    "\n",
    "    # Language-specific boundary detection\n",
    "    boundaries_src = get_phrase_boundaries(doc_src)\n",
    "    boundaries_tgt = get_phrase_boundaries(doc_tgt)  # still works for spanish\n",
    "\n",
    "    chunks_src = split_doc_by_token_positions(doc_src, boundaries_src)\n",
    "    print(chunks_src)\n",
    "    chunks_tgt = split_doc_by_token_positions(doc_tgt, boundaries_tgt)\n",
    "    print(chunks_tgt)\n",
    "\n",
    "    # Edge handling: make sure chunk counts match\n",
    "    if not check_chunk_alignment(chunks_src, chunks_tgt):\n",
    "        return phrase_zh  # no switching point\n",
    "\n",
    "    cs_phrase = \"\"\n",
    "\n",
    "    # Random code-switch one of the chunks (except first)\n",
    "    chunk_n = random.randrange(1, len(chunks_src)-1)\n",
    "    print(f\"Chunk n: {chunk_n}\")\n",
    "    for i in range(len(chunks_src)):\n",
    "        cs_phrase += chunks_tgt[i] if i == chunk_n else chunks_src[i]\n",
    "\n",
    "    cs_phrase = check_and_fix_duplicate_punctuation(remove_spaces_between_chinese_and_punctuation(clean(cs_phrase)))\n",
    "\n",
    "    return cs_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8ba6a-0005-4f83-bd55-5217c84d5d4a",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4f1a92-fcc0-4738-8e3a-c211223a8ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Vir\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.755 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "phrase_es = \"La semana pasada fui de compras con mi suegra y terminé gastando mucho dinero, pero compré cosas muy chulas.\"\n",
    "phrase_zh = add_spaces_to_chinese(\"上周我和岳母一起去购物，花了很多钱，但我买了一些很酷的东西。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "714143a1-b746-410a-9329-ab24c0e0c939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上周   我   和   岳母   一起   去   购物   ', '，   花   了   很多   钱   ', '，   但   我   买   了   一些   很酷   的   东西   ', '。']\n",
      "['La semana pasada fui de compras con mi suegra y terminé gastando mucho dinero', ', pero compré cosas muy chulas', '.']\n",
      "Not same number of chunks\n",
      "上周   我   和   岳母   一起   去   购物   ，   花   了   很多   钱   ，   但   我   买   了   一些   很酷   的   东西   。\n"
     ]
    }
   ],
   "source": [
    "cs = code_switch_phrase_bilingual(phrase_zh, phrase_es)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e04ff52-bde1-4246-a3a1-8164230b0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_es = \"Last summer I went on vacation with my family to Spain, but although the weather was nice, the heat was unbearable.\"\n",
    "phrase_zh = add_spaces_to_chinese(\"去年夏天我和家人去西班牙度假，虽然天气很好，但是却很热。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a820920e-a035-4125-bfb5-fc36406b66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['去年   夏天   我   和   家人   去   西班牙   度假   ，   ', '虽然   天气   很   好   ', '，   但是   却   很   热   ', '。']\n",
      "['Last summer I went on vacation with my family to Spain, but although the weather was nice, the heat was unbearable', '.']\n",
      "Not same number of chunks\n",
      "去年   夏天   我   和   家人   去   西班牙   度假   ，   虽然   天气   很   好   ，   但是   却   很   热   。\n"
     ]
    }
   ],
   "source": [
    "cs = code_switch_phrase_bilingual(phrase_zh, phrase_es)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62fc0f7d-ca0a-4485-9845-7a7371a3695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_es = \"If life gives you lemons, make lemonade.\"\n",
    "phrase_zh = add_spaces_to_chinese(\"如果生活给你柠檬，那就把它做成柠檬水。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0334fa5f-16fa-41b6-80a3-5fbf828093dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '如果   生活   给   你   柠檬   ', '，   那   就   把   它   做成   柠檬水   ', '。']\n",
      "['If life gives you lemons, make lemonade', '.']\n",
      "Not same number of chunks\n",
      "如果   生活   给   你   柠檬   ，   那   就   把   它   做成   柠檬水   。\n"
     ]
    }
   ],
   "source": [
    "cs = code_switch_phrase_bilingual(phrase_zh, phrase_es)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dd215-6f43-427a-8417-2ce3e5277c13",
   "metadata": {},
   "source": [
    "## WORD-LEVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0035a-04f9-4b63-9690-81efaf83f9c5",
   "metadata": {},
   "source": [
    "Extract potential phrases and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3badad-ff6d-4980-8db4-71c94cedd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract multiword and individual POS phrases (adapted for Chinese)\n",
    "def extract_phrases(doc):\n",
    "    phrases = []\n",
    "    used = set()\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        tok = doc[i]\n",
    "\n",
    "        # Manually extract noun phrases (>=2 consecutive NOUN/PROPN)\n",
    "        if tok.pos_ in {\"NOUN\", \"PROPN\"}:\n",
    "            start = i\n",
    "            while i + 1 < len(doc) and doc[i + 1].pos_ in {\"NOUN\", \"PROPN\"}:\n",
    "                i += 1\n",
    "            if i > start:\n",
    "                indices = list(range(start, i + 1))\n",
    "                phrases.append((\"NOUN\", doc[start:i + 1].text, len(indices), indices))\n",
    "                used.update(indices)\n",
    "            else:\n",
    "                if tok.i not in used:\n",
    "                    phrases.append((tok.pos_, tok.text, 1, [tok.i]))\n",
    "                    used.add(tok.i)\n",
    "        # Handle other POS (single words for VERB, ADJ, etc.)\n",
    "        elif tok.pos_ in {\"VERB\", \"ADJ\"} and tok.i not in used:\n",
    "            phrases.append((tok.pos_, tok.text, 1, [tok.i]))\n",
    "            used.add(tok.i)\n",
    "        i += 1\n",
    "\n",
    "    # Extract AUX+VERB pairs\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"AUX\" and tok.head.pos_ == \"VERB\":\n",
    "            pair = sorted([tok.i, tok.head.i])\n",
    "            if not any(i in used for i in pair):\n",
    "                phrases.append((\"VERB\", f\"{doc[pair[0]].text}{doc[pair[1]].text}\", 2, pair))\n",
    "                used.update(pair)\n",
    "\n",
    "    return phrases\n",
    "\n",
    "# Clean spacing (safe for Chinese and spanish)\n",
    "def clean(text):\n",
    "    text = re.sub(r'\\s+([?.!,])', r'\\1', text)  # spanish punctuation\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7f6fc-1498-4a5f-8fc3-8e641257da7d",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2731745-7a12-4c86-aab6-500079232382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def match_capitalization(src: str, tgt: str) -> str:\n",
    "    \"\"\"Ensure tgt phrase follows the capitalization pattern of src phrase.\"\"\"\n",
    "    if not src or not tgt:\n",
    "        return tgt\n",
    "\n",
    "    if src[0].isupper():\n",
    "        return tgt[0].upper() + tgt[1:]\n",
    "    else:\n",
    "        return tgt[0].lower() + tgt[1:]\n",
    "        \n",
    "# Main function\n",
    "def code_switch_word_bilingual(chinese, spanish):\n",
    "    phrase_zh = add_spaces_to_chinese(chinese)\n",
    "    \n",
    "    doc_zh = nlp_zh(chinese)\n",
    "    doc_es = nlp_es(spanish)\n",
    "\n",
    "    zh_tokens = [t.text for t in doc_zh]\n",
    "    es_tokens = [t.text for t in doc_es]\n",
    "\n",
    "    alignment = aligner(\" \".join(zh_tokens), \" \".join(es_tokens))\n",
    "    align_dict = {}\n",
    "    for i, j in alignment:\n",
    "        align_dict.setdefault(i, []).append(j)\n",
    "\n",
    "    #print(align_dict)\n",
    "\n",
    "    phrases = extract_phrases(doc_zh)\n",
    "    total_words = sum(p[2] for p in phrases)\n",
    "    sw_rate = random.uniform(0.15, 0.45)\n",
    "    max_words = max(1, int(sw_rate * total_words))\n",
    "\n",
    "    weights = {\"NOUN\": 0.4, \"VERB\": 0.3, \"ADJ\": 0.2, \"PROPN\": 0.1}\n",
    "    pool = sum([[p]*int(weights[p[0]]*100) for p in phrases if p[0] in weights], [])\n",
    "    random.shuffle(pool)\n",
    "\n",
    "    used = set()\n",
    "    selected = []\n",
    "    replaced = zh_tokens.copy()\n",
    "    selected_count = 0\n",
    "\n",
    "    while pool and selected_count < max_words:\n",
    "        pos, text, length, indices = pool.pop(0)\n",
    "        if any(i in used for i in indices):\n",
    "            continue\n",
    "\n",
    "        tgt_idxs = sorted(set(j for i in indices if i in align_dict for j in align_dict[i]))\n",
    "        if not tgt_idxs:\n",
    "            continue\n",
    "        #print(zh_tokens)\n",
    "        #print(es_tokens)\n",
    "        #print(tgt_idxs)\n",
    "        \n",
    "        es_phrase = \" \".join(es_tokens[j] for j in tgt_idxs).strip()\n",
    "        # Check POS and meaningful change\n",
    "        valid_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"}\n",
    "        es_indices = [j for j in tgt_idxs if j < len(doc_es)]\n",
    "        es_words_pos = [(doc_es[j].text, doc_es[j].pos_) for j in es_indices]\n",
    "        zh_words = [doc_zh[i].text.lower() for i in indices]\n",
    "        \n",
    "        # Only proceed if at least one aligned spanish content word differs from chinese\n",
    "        has_meaningful_change = any(\n",
    "            pos in valid_pos and word.lower() not in zh_words\n",
    "            for word, pos in es_words_pos\n",
    "        )\n",
    "        \n",
    "        if not has_meaningful_change:\n",
    "            continue\n",
    "        if pos == \"PROPN\" and es_phrase.lower() == text.lower():\n",
    "            continue\n",
    "        if len(es_phrase.strip().split()) == 0:\n",
    "            continue\n",
    "\n",
    "        replaced[indices[0]] = match_capitalization(text,es_phrase)\n",
    "        for i in indices[1:]:\n",
    "            replaced[i] = \"\"\n",
    "        used.update(indices)\n",
    "        selected.append((pos, text, es_phrase))\n",
    "        selected_count += length\n",
    "\n",
    "    cs_sentence = clean(\" \".join(replaced))\n",
    "\n",
    "    cs_sentence = check_and_fix_duplicate_punctuation(remove_spaces_between_chinese_and_punctuation(clean(cs_sentence)))\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"chinese\": chinese,\n",
    "        \"spanish\": spanish,\n",
    "        \"cs_sentence\": cs_sentence,\n",
    "        \"selected\": selected,\n",
    "        \"pos_tags\": [(tok.text, tok.pos_) for tok in doc_zh if tok.is_alpha],\n",
    "        \"sw_rate\": sw_rate\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb1c0f-9e6d-4e45-a900-c0d2c004265d",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5154c4f6-703f-41a4-92e0-63d924e02b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def append_to_cs_dataset(df, entry):\n",
    "    \"\"\"\n",
    "    Appends a new code-switched QA entry to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "    - entry: dict with keys 'id', 'context_es', 'context_zh', 'es_q', 'zh_q', 'cs_q', 'sw_rate', 'answers_es', 'answers_zh'\n",
    "    \n",
    "    Returns:\n",
    "    - Updated pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    expected_cols = ['id', 'context_es', 'context_zh', 'es_q', 'zh_q', 'cs_q', 'sw_rate', 'answers_es', 'answers_zh']\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=expected_cols)\n",
    "\n",
    "    # Validate all keys are present\n",
    "    for col in expected_cols:\n",
    "        if col not in entry:\n",
    "            raise ValueError(f\"Missing required field: '{col}'\")\n",
    "\n",
    "    # Append entry\n",
    "    df = pd.concat([df, pd.DataFrame([entry])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fc494-9214-4454-a01f-66d792b7870e",
   "metadata": {},
   "source": [
    "## mMARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b71ed5cd-0038-4359-8f3b-8242f0631c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ZH+ES -> CS:   0%|                                                                           | 0/25000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "ZH+ES -> CS: 100%|█████████████████████████████████████████████████████████████| 25000/25000 [6:26:45<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 25,000 ZH+ES+CS rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build_cs_from_extracted_queries.py\n",
    "# Assumes you already created: mmarco_common_3langs/joined_queries_common.jsonl\n",
    "# (with fields: query_id, en.text, es.text, ...)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- where your extracted files live ---\n",
    "OUTDIR = Path(\"IR\")\n",
    "JOINED_QUERIES = OUTDIR / \"joined_queries.common.selection.jsonl\"\n",
    "\n",
    "# --- your function must be available ---\n",
    "# from your_module import code_switch_word_bilingual\n",
    "# expected return keys: \"spanish\", \"spanish\", \"selected\", \"cs_sentence\", \"sw_rate\"\n",
    "\n",
    "def iter_jsonl(path, desc=None):\n",
    "    # 1) pre-count non-empty lines so tqdm can show %\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        total = sum(1 for line in f if line.strip())\n",
    "\n",
    "    # 2) iterate with tqdm using that total\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=total, desc=desc):\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# now call WITHOUT wrapping in an extra tqdm\n",
    "for rec in iter_jsonl(JOINED_QUERIES, desc=\"ZH+ES -> CS\"):\n",
    "    qid = rec.get(\"query_id\")\n",
    "    q_es = (rec.get(\"es\") or {}).get(\"text\")\n",
    "    q_zh = (rec.get(\"zh\") or {}).get(\"text\")\n",
    "\n",
    "    # safety: skip if either side is missing (shouldn't happen with 'joined' file)\n",
    "    if not q_es or not q_zh:\n",
    "        continue\n",
    "\n",
    "    # Run your code-switcher\n",
    "    result = code_switch_word_bilingual(q_zh, q_es) or {}\n",
    "    cs_q = result.get(\"cs_sentence\")\n",
    "    sw_rate = result.get(\"sw_rate\", 0.0)\n",
    "\n",
    "    new_entry = {\n",
    "        \"id\": qid,\n",
    "        \"esp_q\": q_es,\n",
    "        \"zh_q\": q_zh,\n",
    "        \"cs_q\": cs_q,\n",
    "        \"sw_rate\": round(sw_rate, 3)\n",
    "    }\n",
    "    #print(new_entry)\n",
    "    rows.append(new_entry)\n",
    "\n",
    "# Build DataFrame and (optionally) save\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"id\", \"esp_q\", \"zh_q\", \"cs_q\", \"sw_rate\"\n",
    "])\n",
    "\n",
    "print(f\"Built {len(df):,} ZH+ES+CS rows\")\n",
    "# df.to_csv(OUTDIR / \" .csv\", index=False, encoding=\"utf-8\")\n",
    "# df.to_json(OUTDIR / \"queries_zh_en_cs.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deccee49-5f86-4ca1-a2fd-a0fa54569b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>esp_q</th>\n",
       "      <th>zh_q</th>\n",
       "      <th>cs_q</th>\n",
       "      <th>sw_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1095807</td>\n",
       "      <td>cuanto es el impuesto a las ventas en rancho c...</td>\n",
       "      <td>兰乔科尔多瓦的销售税是多少</td>\n",
       "      <td>兰乔科尔多瓦的impuesto ventas是多少</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1082615</td>\n",
       "      <td>¿Qué representa el cráneo en lof?</td>\n",
       "      <td>头骨在lof中代表什么</td>\n",
       "      <td>cráneo在lof中代表什么</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094034</td>\n",
       "      <td>es rexius inc. una LLC?</td>\n",
       "      <td>是雷克修斯公司有限责任公司？</td>\n",
       "      <td>是雷克修斯公司有限. LLC ？</td>\n",
       "      <td>0.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>826805</td>\n",
       "      <td>¿Qué está protegiendo la rodilla?</td>\n",
       "      <td>什么是膝跳保护</td>\n",
       "      <td>什么是rodilla保护</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1082848</td>\n",
       "      <td>que significa el acrónimo ctfu</td>\n",
       "      <td>缩写ctfu是什么意思</td>\n",
       "      <td>acrónimo ctfu是什么意思</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>853064</td>\n",
       "      <td>¿Cuál es el voltaje de una batería tesla?</td>\n",
       "      <td>特斯拉电池组的电压是多少</td>\n",
       "      <td>特斯拉batería的电压是多少</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1101867</td>\n",
       "      <td>wilmer valderrama vale</td>\n",
       "      <td>威尔默瓦尔德拉玛值得</td>\n",
       "      <td>wilmer valderrama值得</td>\n",
       "      <td>0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>109892</td>\n",
       "      <td>costo de instalación de material y mano de obr...</td>\n",
       "      <td>詹姆斯·哈迪搭板壁板的安装材料和人工成本</td>\n",
       "      <td>詹姆斯·哈迪revestimiento壁板的instalación material和人工成本</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>421846</td>\n",
       "      <td>es prunus evereste un arbolito</td>\n",
       "      <td>李子树是一棵小树吗</td>\n",
       "      <td>李子树evereste一棵小树吗</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>399436</td>\n",
       "      <td>es una cerveza al día mala para ti</td>\n",
       "      <td>每天喝一杯啤酒对你有害吗</td>\n",
       "      <td>每天喝一杯cerveza对你有害吗</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              esp_q  \\\n",
       "0      1095807  cuanto es el impuesto a las ventas en rancho c...   \n",
       "1      1082615                  ¿Qué representa el cráneo en lof?   \n",
       "2      1094034                            es rexius inc. una LLC?   \n",
       "3       826805                  ¿Qué está protegiendo la rodilla?   \n",
       "4      1082848                     que significa el acrónimo ctfu   \n",
       "...        ...                                                ...   \n",
       "24995   853064          ¿Cuál es el voltaje de una batería tesla?   \n",
       "24996  1101867                             wilmer valderrama vale   \n",
       "24997   109892  costo de instalación de material y mano de obr...   \n",
       "24998   421846                     es prunus evereste un arbolito   \n",
       "24999   399436                 es una cerveza al día mala para ti   \n",
       "\n",
       "                       zh_q                                             cs_q  \\\n",
       "0             兰乔科尔多瓦的销售税是多少                        兰乔科尔多瓦的impuesto ventas是多少   \n",
       "1               头骨在lof中代表什么                                  cráneo在lof中代表什么   \n",
       "2            是雷克修斯公司有限责任公司？                                 是雷克修斯公司有限. LLC ？   \n",
       "3                   什么是膝跳保护                                     什么是rodilla保护   \n",
       "4               缩写ctfu是什么意思                               acrónimo ctfu是什么意思   \n",
       "...                     ...                                              ...   \n",
       "24995          特斯拉电池组的电压是多少                                 特斯拉batería的电压是多少   \n",
       "24996            威尔默瓦尔德拉玛值得                              wilmer valderrama值得   \n",
       "24997  詹姆斯·哈迪搭板壁板的安装材料和人工成本  詹姆斯·哈迪revestimiento壁板的instalación material和人工成本   \n",
       "24998             李子树是一棵小树吗                                 李子树evereste一棵小树吗   \n",
       "24999          每天喝一杯啤酒对你有害吗                                每天喝一杯cerveza对你有害吗   \n",
       "\n",
       "       sw_rate  \n",
       "0        0.312  \n",
       "1        0.163  \n",
       "2        0.322  \n",
       "3        0.266  \n",
       "4        0.179  \n",
       "...        ...  \n",
       "24995    0.355  \n",
       "24996    0.246  \n",
       "24997    0.385  \n",
       "24998    0.422  \n",
       "24999    0.385  \n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d611d93-84fd-4c92-9097-f68f91a5cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(OUTDIR / \"mMARCO_queries_zh_es_cs_selection.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (embedding)",
   "language": "python",
   "name": "embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
