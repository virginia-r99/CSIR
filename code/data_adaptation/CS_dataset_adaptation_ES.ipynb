{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ebc971-7377-4ef5-b0ee-1543e7910ede",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GENERAL FUNCTIONS AND INITIALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccfcf41-f307-4372-873d-3d0d2661a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f99a5-c1c2-4f5a-9d83-32bf962c17e2",
   "metadata": {},
   "source": [
    "Spacy NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f419689-fdb1-46a1-bdb4-41e74915ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a664f2-b531-4e25-b592-230d6eac54c9",
   "metadata": {},
   "source": [
    "Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d3cef3-a55c-4743-b21d-797f18b94978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model = AutoModel.from_pretrained(\"aneuraz/awesome-align-with-co\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aneuraz/awesome-align-with-co\")\n",
    "\n",
    "def aligner(src, tgt):\n",
    "    # model parameters\n",
    "    align_layer = 8\n",
    "    threshold = 1e-3\n",
    "    \n",
    "    # pre-processing\n",
    "    sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
    "    token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
    "    wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
    "    ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
    "    sub2word_map_src = []\n",
    "    for i, word_list in enumerate(token_src):\n",
    "      sub2word_map_src += [i for x in word_list]\n",
    "    sub2word_map_tgt = []\n",
    "    for i, word_list in enumerate(token_tgt):\n",
    "      sub2word_map_tgt += [i for x in word_list]\n",
    "      \n",
    "    # alignment\n",
    "    align_layer = 8\n",
    "    threshold = 1e-3\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "      out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "    \n",
    "      dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
    "    \n",
    "      softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
    "      softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
    "    \n",
    "      softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
    "    \n",
    "    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
    "    align_words = set()\n",
    "    for i, j in align_subwords:\n",
    "      align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
    "\n",
    "    sorted_align_words = sorted(align_words, key=lambda x: x[0])\n",
    "      \n",
    "    return sorted_align_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5464b0-61b4-44fb-8ea6-b492c6afd360",
   "metadata": {},
   "source": [
    "Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376192c-fa08-4fcd-b87f-7151e5ebeac1",
   "metadata": {},
   "source": [
    "# INTRA-SWITCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ebb52-ac7b-4224-b716-9d1ac816e417",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PHRASE-LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d189ebbc-0b3e-4900-a28e-0edfad4d1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_boundaries(doc):\n",
    "    # Find 'CCONJ' tokens that join phrases (not simple compound nouns)\n",
    "    boundaries = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ' or token.pos_ == 'PUNCT':\n",
    "            # If the token is connecting ROOTs or verbs, it's phrase-separating\n",
    "            if (token.head.pos_ in {'VERB', 'AUX'} or token.head.dep_ == 'ROOT') and (token.i)-1 not in boundaries:\n",
    "                print(f\"'{token.text}' at position {token.i} separates phrases.\")\n",
    "                boundaries.append(token.i)\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80587200-df4c-44a6-849b-0e59c288c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_by_token_positions(doc, positions):\n",
    "    \"\"\"\n",
    "    Split a spaCy Doc object at specified token positions.\n",
    "\n",
    "    Parameters:\n",
    "        doc (spacy.tokens.Doc): Tokenized spaCy Doc object.\n",
    "        positions (List[int]): Token indices (including punctuation).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of string chunks split at token boundaries.\n",
    "    \"\"\"\n",
    "    tokens = [token.text_with_ws for token in doc]  # Preserve spacing and punctuation\n",
    "    chunks = []\n",
    "    prev = 0\n",
    "    for pos in positions:\n",
    "        chunks.append(''.join(tokens[prev:pos]))\n",
    "        prev = pos\n",
    "    chunks.append(''.join(tokens[prev:]))  # Add last segment\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29550195-7afb-4a0d-854a-e03c6355ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_switch_phrase_bilingual(phrase_es, phrase_en):\n",
    "    # Process source sentence \n",
    "    doc_src = nlp_es(phrase_es)\n",
    "    print(str([(token.text, token.pos_) for token in doc_src]))\n",
    "    \n",
    "    # Process target sentence\n",
    "    doc_tgt = nlp_en(phrase_en)\n",
    "    print(str([(token.text, token.pos_) for token in doc_tgt]))\n",
    "    \n",
    "    # Find 'CCONJ' tokens that join phrases (not simple compound nouns)\n",
    "    conjs_src = get_phrase_boundaries(doc_src)\n",
    "    \n",
    "    conjs_tgt = get_phrase_boundaries(doc_tgt)\n",
    "\n",
    "    chunks_src = split_doc_by_token_positions(doc_src, conjs_src)\n",
    "\n",
    "    #print(\"Source chunks:\")\n",
    "    #for i, chunk in enumerate(chunks_src):\n",
    "    #    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    \n",
    "    chunks_tgt = split_doc_by_token_positions(doc_tgt, conjs_tgt)\n",
    "    \n",
    "    #print(\"Target chunks:\")\n",
    "    #for i, chunk in enumerate(chunks_tgt):\n",
    "    #    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    \n",
    "    cs_phrase = \"\"\n",
    "    \n",
    "    # If there's only 2 chunks, translate the second one\n",
    "    if len(chunks) == 2:\n",
    "        # Translate chunk\n",
    "        cs_phrase = chunks_src[0]+chunks_tgt[1]\n",
    "        \n",
    "    # If there's more than 2 chunks, random select between chunks (except 1st)\n",
    "    elif len(chunks) > 2:\n",
    "        chunk_n = random.randrange(1, len(chunks_src))\n",
    "        # Translate chunk\n",
    "        cs_phrase = chunks_src[0]\n",
    "        for i in range(1,len(chunks)):\n",
    "            if i == chunk_n:\n",
    "                cs_phrase = cs_phrase+chunks_tgt[i]\n",
    "            else:\n",
    "                cs_phrase = cs_phrase+chunks_src[i]\n",
    "\n",
    "    return cs_phrase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dd215-6f43-427a-8417-2ce3e5277c13",
   "metadata": {},
   "source": [
    "## WORD-LEVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0035a-04f9-4b63-9690-81efaf83f9c5",
   "metadata": {},
   "source": [
    "Extract potential phrases and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3badad-ff6d-4980-8db4-71c94cedd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract multiword and individual POS phrases\n",
    "def extract_phrases(doc):\n",
    "    phrases = []\n",
    "    used = set()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk) >= 2:\n",
    "            indices = [tok.i for tok in chunk]\n",
    "            phrases.append((\"NOUN\", chunk.text, len(indices), indices))\n",
    "            used.update(indices)\n",
    "\n",
    "    for i, tok in enumerate(doc):\n",
    "        if tok.pos_ == \"AUX\" and tok.head.pos_ == \"VERB\":\n",
    "            pair = sorted([tok.i, tok.head.i])\n",
    "            if len(pair) == 2 and not any(i in used for i in pair):\n",
    "                phrases.append((\"VERB\", f\"{doc[pair[0]].text} {doc[pair[1]].text}\", 2, pair))\n",
    "                used.update(pair)\n",
    "\n",
    "    for tok in doc:\n",
    "        if tok.i in used: continue\n",
    "        if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"}:\n",
    "            phrases.append((tok.pos_, tok.text, 1, [tok.i]))\n",
    "\n",
    "    return phrases\n",
    "\n",
    "# Clean spacing\n",
    "def clean(text):\n",
    "    text = re.sub(r'\\s+([?.!,])', r'\\1', text)\n",
    "    return re.sub(r'\\s{2,}', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7f6fc-1498-4a5f-8fc3-8e641257da7d",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2731745-7a12-4c86-aab6-500079232382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_capitalization(src: str, tgt: str) -> str:\n",
    "    \"\"\"Ensure tgt phrase follows the capitalization pattern of src phrase.\"\"\"\n",
    "    if not src or not tgt:\n",
    "        return tgt\n",
    "\n",
    "    if src[0].isupper():\n",
    "        return tgt[0].upper() + tgt[1:]\n",
    "    else:\n",
    "        return tgt[0].lower() + tgt[1:]\n",
    "        \n",
    "# Main function\n",
    "def code_switch_word_bilingual(spanish, english):\n",
    "    doc_es = nlp_es(spanish)\n",
    "    doc_en = nlp_en(english)\n",
    "\n",
    "    es_tokens = [t.text for t in doc_es]\n",
    "    en_tokens = [t.text for t in doc_en]\n",
    "\n",
    "    alignment = aligner(\" \".join(es_tokens), \" \".join(en_tokens))\n",
    "    align_dict = {}\n",
    "    for i, j in alignment:\n",
    "        align_dict.setdefault(i, []).append(j)\n",
    "\n",
    "    phrases = extract_phrases(doc_es)\n",
    "    total_words = sum(p[2] for p in phrases)\n",
    "    sw_rate = random.uniform(0.15, 0.45)\n",
    "    max_words = max(1, int(sw_rate * total_words))\n",
    "\n",
    "    weights = {\"NOUN\": 0.4, \"VERB\": 0.3, \"ADJ\": 0.2, \"PROPN\": 0.1}\n",
    "    pool = sum([[p]*int(weights[p[0]]*100) for p in phrases if p[0] in weights], [])\n",
    "    random.shuffle(pool)\n",
    "\n",
    "    used = set()\n",
    "    selected = []\n",
    "    replaced = es_tokens.copy()\n",
    "    selected_count = 0\n",
    "\n",
    "    while pool and selected_count < max_words:\n",
    "        pos, text, length, indices = pool.pop(0)\n",
    "        if any(i in used for i in indices):\n",
    "            continue\n",
    "\n",
    "        tgt_idxs = sorted(set(j for i in indices if i in align_dict for j in align_dict[i]))\n",
    "        if not tgt_idxs:\n",
    "            continue\n",
    "\n",
    "        eng_phrase = \" \".join(en_tokens[j] for j in tgt_idxs).strip()\n",
    "        # Check POS and meaningful change\n",
    "        valid_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"}\n",
    "        eng_indices = [j for j in tgt_idxs if j < len(doc_en)]\n",
    "        eng_words_pos = [(doc_en[j].text, doc_en[j].pos_) for j in eng_indices]\n",
    "        es_words = [doc_es[i].text.lower() for i in indices]\n",
    "        \n",
    "        # Only proceed if at least one aligned English content word differs from Spanish\n",
    "        has_meaningful_change = any(\n",
    "            pos in valid_pos and word.lower() not in es_words\n",
    "            for word, pos in eng_words_pos\n",
    "        )\n",
    "        \n",
    "        if not has_meaningful_change:\n",
    "            continue\n",
    "        if pos == \"PROPN\" and eng_phrase.lower() == text.lower():\n",
    "            continue\n",
    "        if len(eng_phrase.strip().split()) == 0:\n",
    "            continue\n",
    "\n",
    "        replaced[indices[0]] = match_capitalization(text,eng_phrase)\n",
    "        for i in indices[1:]:\n",
    "            replaced[i] = \"\"\n",
    "        used.update(indices)\n",
    "        selected.append((pos, text, eng_phrase))\n",
    "        selected_count += length\n",
    "\n",
    "    cs_sentence = clean(\" \".join(replaced))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"spanish\": spanish,\n",
    "        \"english\": english,\n",
    "        \"cs_sentence\": cs_sentence,\n",
    "        \"selected\": selected,\n",
    "        \"pos_tags\": [(tok.text, tok.pos_) for tok in doc_es if tok.is_alpha],\n",
    "        \"sw_rate\": sw_rate\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1063af-48e8-40b8-93e3-ef8ad3fd31cb",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a127dc-6e02-4b60-8cef-cdf605df3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def append_to_cs_dataset(df, entry):\n",
    "    \"\"\"\n",
    "    Appends a new code-switched QA entry to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "    - entry: dict with keys 'id', 'context', 'eng_q', 'spa_q', 'cs_q', 'sw_rate', 'answers'\n",
    "    \n",
    "    Returns:\n",
    "    - Updated pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    expected_cols = ['id', 'context_en', 'context_es', 'eng_q', 'spa_q', 'cs_q', 'sw_rate', 'answers_en', 'answers_es']\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=expected_cols)\n",
    "\n",
    "    # Validate all keys are present\n",
    "    for col in expected_cols:\n",
    "        if col not in entry:\n",
    "            raise ValueError(f\"Missing required field: '{col}'\")\n",
    "\n",
    "    # Append entry\n",
    "    df = pd.concat([df, pd.DataFrame([entry])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67b06f-ce5c-4efc-b7cc-720f3e7d0486",
   "metadata": {},
   "source": [
    "## mMARCO adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9323d091-a2d5-4a42-9560-03328fd33a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ES+EN -> CS:   0%|                                                                           | 0/25000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "ES+EN -> CS: 100%|█████████████████████████████████████████████████████████████| 25000/25000 [5:36:35<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 25,000 ES+EN+CS rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build_cs_from_extracted_queries.py\n",
    "# Assumes you already created: mmarco_common_3langs/joined_queries_common.jsonl\n",
    "# (with fields: query_id, en.text, es.text, ...)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- where your extracted files live ---\n",
    "OUTDIR = Path(\"IR\")\n",
    "JOINED_QUERIES = OUTDIR / \"joined_queries.common.selection.jsonl\"\n",
    "\n",
    "# --- your function must be available ---\n",
    "# from your_module import code_switch_word_bilingual\n",
    "# expected return keys: \"spanish\", \"english\", \"selected\", \"cs_sentence\", \"sw_rate\"\n",
    "\n",
    "def iter_jsonl(path, desc=None):\n",
    "    # 1) pre-count non-empty lines so tqdm can show %\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        total = sum(1 for line in f if line.strip())\n",
    "\n",
    "    # 2) iterate with tqdm using that total\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=total, desc=desc):\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# now call WITHOUT wrapping in an extra tqdm\n",
    "for rec in iter_jsonl(JOINED_QUERIES, desc=\"ES+EN -> CS\"):\n",
    "    qid = rec.get(\"query_id\")\n",
    "    q_en = (rec.get(\"en\") or {}).get(\"text\")\n",
    "    q_es = (rec.get(\"es\") or {}).get(\"text\")\n",
    "\n",
    "    # safety: skip if either side is missing (shouldn't happen with 'joined' file)\n",
    "    if not q_en or not q_es:\n",
    "        continue\n",
    "\n",
    "    # Run your code-switcher\n",
    "    result = code_switch_word_bilingual(q_es, q_en) or {}\n",
    "    cs_q = result.get(\"cs_sentence\")\n",
    "    sw_rate = result.get(\"sw_rate\", 0.0)\n",
    "\n",
    "    new_entry = {\n",
    "        \"id\": qid,\n",
    "        \"eng_q\": q_en,\n",
    "        \"spa_q\": q_es,\n",
    "        \"cs_q\": cs_q,\n",
    "        \"sw_rate\": round(sw_rate, 3)\n",
    "    }\n",
    "    #print(new_entry)\n",
    "    rows.append(new_entry)\n",
    "\n",
    "# Build DataFrame and (optionally) save\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"id\", \"eng_q\", \"spa_q\", \"cs_q\", \"sw_rate\"\n",
    "])\n",
    "\n",
    "print(f\"Built {len(df):,} ES+EN+CS rows\")\n",
    "# df.to_csv(OUTDIR / \" .csv\", index=False, encoding=\"utf-8\")\n",
    "# df.to_json(OUTDIR / \"queries_es_en_cs.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "595610d9-9e9f-429e-bd87-0b0902bcea6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>eng_q</th>\n",
       "      <th>spa_q</th>\n",
       "      <th>cs_q</th>\n",
       "      <th>sw_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1095807</td>\n",
       "      <td>how much is sales tax in rancho cordova</td>\n",
       "      <td>cuanto es el impuesto a las ventas en rancho c...</td>\n",
       "      <td>cuanto es el impuesto a sales en rancho cordova</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1082615</td>\n",
       "      <td>what does the skull represent in lof</td>\n",
       "      <td>¿Qué representa el cráneo en lof?</td>\n",
       "      <td>¿ Qué represent el cráneo en lof?</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094034</td>\n",
       "      <td>is rexius inc. an llc?</td>\n",
       "      <td>es rexius inc. una LLC?</td>\n",
       "      <td>es rexius inc. una LLC?</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>826805</td>\n",
       "      <td>what is the knee jerk protecting</td>\n",
       "      <td>¿Qué está protegiendo la rodilla?</td>\n",
       "      <td>¿ Qué está protegiendo the knee jerk?</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1082848</td>\n",
       "      <td>what does the acronym ctfu mean</td>\n",
       "      <td>que significa el acrónimo ctfu</td>\n",
       "      <td>que significa the acronym ctfu</td>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>853064</td>\n",
       "      <td>what is the voltage of a tesla battery pack</td>\n",
       "      <td>¿Cuál es el voltaje de una batería tesla?</td>\n",
       "      <td>¿ Cuál es el voltaje de a tesla battery?</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1101867</td>\n",
       "      <td>wilmer valderrama worth</td>\n",
       "      <td>wilmer valderrama vale</td>\n",
       "      <td>wilmer valderrama worth</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>109892</td>\n",
       "      <td>cost to install material and labor for james h...</td>\n",
       "      <td>costo de instalación de material y mano de obr...</td>\n",
       "      <td>cost de instalación de material y mano de labo...</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>421846</td>\n",
       "      <td>is prunus evereste a small tree</td>\n",
       "      <td>es prunus evereste un arbolito</td>\n",
       "      <td>es prunus evereste a tree</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>399436</td>\n",
       "      <td>is a beer per day bad for you</td>\n",
       "      <td>es una cerveza al día mala para ti</td>\n",
       "      <td>is a beer al día mala para ti</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              eng_q  \\\n",
       "0      1095807            how much is sales tax in rancho cordova   \n",
       "1      1082615               what does the skull represent in lof   \n",
       "2      1094034                             is rexius inc. an llc?   \n",
       "3       826805                   what is the knee jerk protecting   \n",
       "4      1082848                    what does the acronym ctfu mean   \n",
       "...        ...                                                ...   \n",
       "24995   853064        what is the voltage of a tesla battery pack   \n",
       "24996  1101867                            wilmer valderrama worth   \n",
       "24997   109892  cost to install material and labor for james h...   \n",
       "24998   421846                    is prunus evereste a small tree   \n",
       "24999   399436                      is a beer per day bad for you   \n",
       "\n",
       "                                                   spa_q  \\\n",
       "0      cuanto es el impuesto a las ventas en rancho c...   \n",
       "1                      ¿Qué representa el cráneo en lof?   \n",
       "2                                es rexius inc. una LLC?   \n",
       "3                      ¿Qué está protegiendo la rodilla?   \n",
       "4                         que significa el acrónimo ctfu   \n",
       "...                                                  ...   \n",
       "24995          ¿Cuál es el voltaje de una batería tesla?   \n",
       "24996                             wilmer valderrama vale   \n",
       "24997  costo de instalación de material y mano de obr...   \n",
       "24998                     es prunus evereste un arbolito   \n",
       "24999                 es una cerveza al día mala para ti   \n",
       "\n",
       "                                                    cs_q  sw_rate  \n",
       "0        cuanto es el impuesto a sales en rancho cordova    0.436  \n",
       "1                      ¿ Qué represent el cráneo en lof?    0.443  \n",
       "2                                es rexius inc. una LLC?    0.379  \n",
       "3                  ¿ Qué está protegiendo the knee jerk?    0.195  \n",
       "4                         que significa the acronym ctfu    0.407  \n",
       "...                                                  ...      ...  \n",
       "24995           ¿ Cuál es el voltaje de a tesla battery?    0.203  \n",
       "24996                            wilmer valderrama worth    0.370  \n",
       "24997  cost de instalación de material y mano de labo...    0.393  \n",
       "24998                          es prunus evereste a tree    0.270  \n",
       "24999                      is a beer al día mala para ti    0.359  \n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58fa16a-040a-4135-b8ab-df2795d40cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(OUTDIR / \"mMARCO_queries_es_en_cs_selection.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (embedding)",
   "language": "python",
   "name": "embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
