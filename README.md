# Code-Switching IR Evaluation Repository

This repository contains resources, data, and experiments for **evaluating cross-lingual and code-switching information retrieval (CS-IR)** systems.  
It includes datasets, formal ontologies, evaluation scripts, and result summaries for various embedding and LLM-based retrieval models.

> **Note:** Replace variables with your own values wherever applicable (e.g., repository name, author, contact email, environment names, and file paths).

---

## ğŸ“‚ Repository Structure

```
|   LICENSE
|   README.md
|   requirements.txt
|
+---code
|   +---data_adaptation
|   |       CS_dataset_adaptation_ES.ipynb
|   |       CS_dataset_adaptation_ZH-ES.ipynb
|   |       CS_dataset_adaptation_ZH.ipynb
|   |
|   \---evaluation
|           CS-IR_EM_Alignment_Evaluation.ipynb
|           CS-IR_EM_Evaluation.ipynb
|           CS-IR_LLM_Evaluation.ipynb
|
+---data
|       docs_subset.jsonl
|       qrels_subset.jsonl
|       queries_subset.jsonl
|
+---formalisation
|       csir-adapt.ttl
|       csir-data.ttl
|       csir-exec.ttl
|       csir-metric.ttl
|       csir-model.ttl
|       csir-scenario.ttl
|       csir-task.ttl
|
\---results
    |   CS-IR_summary_alignment.xlsx
    |   CS-IR_summary_EMs.xlsx
    |   CS-IR_summary_LLMs.xlsx
    |
    +---EM_evaluation_results
    |   â”œâ”€â”€ Alibaba-NLP__gte-multilingual-base/
    |   â”œâ”€â”€ intfloat__multilingual-e5-base/
    |   â”œâ”€â”€ intfloat__multilingual-e5-large/
    |   â”œâ”€â”€ intfloat__multilingual-e5-small/
    |   â”œâ”€â”€ jinaai__jina-embeddings-v3/
    |   â””â”€â”€ upskyy__bge-m3-korean/
    |
    â””---LLM_evaluation_results
        â”œâ”€â”€ meta-llama__Llama-3.1-8B-Instruct/
        â”œâ”€â”€ meta-llama__Llama-3.2-1B-Instruct/
        â”œâ”€â”€ microsoft__Phi-3-mini-4k-instruct/
        â”œâ”€â”€ Qwen__Qwen2.5-3B-Instruct/
        â””â”€â”€ Qwen__Qwen2.5-7B-Instruct/
```

---

## ğŸ§© Contents Description

### **Root Files**
- **LICENSE** â€” Open-source license governing use of this repository.  
- **README.md** â€” This documentation file.  
- **requirements.txt** â€” Python dependencies required to run the notebooks and scripts.

---

### **`code/`**
Contains the Jupyter notebooks used for data adaptation and evaluation.

#### â–¸ `data_adaptation/`
Notebooks for preparing datasets in different languages and code-switching scenarios:
- `CS_dataset_adaptation_ES.ipynb` â€” Spanish dataset adaptation.
- `CS_dataset_adaptation_ZH.ipynb` â€” Chinese dataset adaptation.
- `CS_dataset_adaptation_ZH-ES.ipynb` â€” Mixed Chineseâ€“Spanish adaptation.

#### â–¸ `evaluation/`
Evaluation scripts and analysis notebooks:
- `CS-IR_EM_Evaluation.ipynb` â€” Embedding model evaluation.
- `CS-IR_EM_Alignment_Evaluation.ipynb` â€” Alignment evaluation across embeddings.
- `CS-IR_LLM_Evaluation.ipynb` â€” LLM-based evaluation of retrieval performance.

---

### **`data/`**
Contains lightweight subsets of the CS-IR dataset used in experiments:
- `docs_subset.jsonl` â€” Document collection.
- `qrels_subset.jsonl` â€” Relevance judgments.
- `queries_subset.jsonl` â€” Query set.

---

### **`formalisation/`**
Semantic representations (RDF/Turtle) defining the CS-IR formalisation and metadata:
- `csir-adapt.ttl` â€” Adaptation and preprocessing formalisation.
- `csir-data.ttl` â€” Dataset description.
- `csir-exec.ttl` â€” Execution and evaluation process.
- `csir-metric.ttl` â€” Metrics used for evaluation.
- `csir-model.ttl` â€” Model specifications.
- `csir-scenario.ttl` â€” Experimental configurations.
- `csir-task.ttl` â€” Task and benchmark formalisation.

---

### **`results/`**
Aggregated outputs and evaluation metrics.  
**Note:** This directory includes **all resulting files produced by our executions**, including raw per-query CSVs and aggregated summaries.

#### â–¸ Summary files
- `CS-IR_summary_alignment.xlsx` â€” Alignment results across embedding models.
- `CS-IR_summary_EMs.xlsx` â€” Summary of embedding model performance.
- `CS-IR_summary_LLMs.xlsx` â€” Summary of LLM-based evaluation results.

#### â–¸ `EM_evaluation_results/`
Detailed CSV evaluation outputs for **embedding models**:  
Each subfolder corresponds to a model (e.g., `intfloat__multilingual-e5-base`), containing multiple datasets:
- `D_ALL_GT3L/` â€” Combined multilingual dataset.
- `D_en/`, `D_es/`, `D_zh/` â€” Monolingual datasets for English, Spanish, and Chinese.

Each folder contains per-query evaluation results (e.g., `Q_en.csv`, `Q_es__to__zh.csv`, etc.) covering both monolingual and cross-lingual retrieval.

#### â–¸ `LLM_evaluation_results/`
Detailed evaluation results for **large language models** used as retrieval evaluators:  
Each subfolder (e.g., `meta-llama__Llama-3.1-8B-Instruct`, `Qwen__Qwen2.5-7B-Instruct`) contains the same multilingual dataset structure as embedding model results.

---

## âš™ï¸ Setup & Usage

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run notebooks**
   Open the notebooks in `code/` and execute sequentially (e.g., data adaptation â†’ embedding evaluation â†’ LLM evaluation).

3. **View results**
   - Summary metrics: see `/results/*.xlsx`
   - Per-model evaluations: explore `/results/EM_evaluation_results/` and `/results/LLM_evaluation_results/`

---

## ğŸ“œ License
This repository is distributed under the terms specified in the `LICENSE` file.

---

## ğŸ“§ Contact
For questions or collaboration, please open an issue or email **virginia.ramon@upm.es**.  

